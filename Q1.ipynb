{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07ca592",
   "metadata": {},
   "source": [
    "Here is a **detailed README** that you can use for your project. It outlines the entire process, from setup to running the app, and provides an overview of the project's components.\n",
    "\n",
    "---\n",
    "\n",
    "# **English to Urdu Translation Model Using Transformer and LSTM**\n",
    "\n",
    "This project demonstrates the implementation of machine translation from **English to Urdu** using two models:\n",
    "- **Transformer Model**\n",
    "- **LSTM Model**\n",
    "\n",
    "The translation models are trained on the **UMC005 English-Urdu Parallel Corpus**, and the system allows users to input English sentences and get translated Urdu sentences. Additionally, the project includes:\n",
    "- Evaluation metrics (BLEU, ROUGE)\n",
    "- Attention visualization for the Transformer model\n",
    "- Pre-trained models that can be used for translation without retraining\n",
    "\n",
    "## **Project Structure**\n",
    "\n",
    "The project is structured as follows:\n",
    "\n",
    "```\n",
    "/Q1\n",
    "├── data/\n",
    "│   ├── bible/\n",
    "│   ├── quran/\n",
    "├── models/\n",
    "│   ├── transformer.py\n",
    "│   ├── lstm.py\n",
    "├── utils/\n",
    "│   ├── tokenizer.py\n",
    "│   ├── dataset.py\n",
    "│   ├── metrics.py\n",
    "│   ├── visualization.py\n",
    "├── app.py\n",
    "├── train_transformer.py\n",
    "├── train_lstm.py\n",
    "├── evaluate.py\n",
    "├── requirements.txt\n",
    "├── README.md\n",
    "└── saved_models/\n",
    "```\n",
    "\n",
    "### **Key Files and Folders**:\n",
    "- **`data/`**: Contains the English and Urdu sentences for training, validation, and testing.\n",
    "- **`models/`**: Contains model implementations for both Transformer (`transformer.py`) and LSTM (`lstm.py`).\n",
    "- **`utils/`**: Contains utility functions:\n",
    "  - **`tokenizer.py`**: Tokenizer implementation for Byte Pair Encoding (BPE).\n",
    "  - **`dataset.py`**: Dataset class for loading and processing data.\n",
    "  - **`metrics.py`**: Functions for calculating evaluation metrics like BLEU and ROUGE.\n",
    "  - **`visualization.py`**: Function for visualizing attention weights.\n",
    "- **`app.py`**: Streamlit interface for inputting text and getting translations, along with model evaluation and attention visualization.\n",
    "- **`train_transformer.py`**: Script to train the Transformer model.\n",
    "- **`train_lstm.py`**: Script to train the LSTM model.\n",
    "- **`evaluate.py`**: Script to evaluate the models on the test dataset using BLEU and ROUGE metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **Installation and Setup**\n",
    "\n",
    "### **1. Install Required Libraries**\n",
    "\n",
    "Before running the project, you need to install the required dependencies. The project uses **PyTorch**, **Streamlit**, and several other libraries.\n",
    "\n",
    "To install the required packages, run the following command:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### **2. Download the Data**\n",
    "\n",
    "You will need to download the **UMC005 English-Urdu Parallel Corpus** for training and evaluation. This corpus is available from the official site. You can place the downloaded data in the `data/` directory.\n",
    "\n",
    "After downloading, the folder structure will look like:\n",
    "\n",
    "```\n",
    "/data\n",
    "├── bible\n",
    "│   ├── train.en\n",
    "│   ├── train.ur\n",
    "│   ├── dev.en\n",
    "│   ├── dev.ur\n",
    "│   ├── test.en\n",
    "│   ├── test.ur\n",
    "├── quran\n",
    "│   ├── train.en\n",
    "│   ├── train.ur\n",
    "│   ├── dev.en\n",
    "│   ├── dev.ur\n",
    "│   ├── test.en\n",
    "│   ├── test.ur\n",
    "```\n",
    "\n",
    "### **3. Pre-trained Models**\n",
    "\n",
    "You can use the pre-trained models stored in the `models/` directory:\n",
    "- **`tokenizer_en.pt`**: English tokenizer\n",
    "- **`tokenizer_ur.pt`**: Urdu tokenizer\n",
    "- **`transformer_model.pth`**: Pre-trained Transformer model\n",
    "- **`lstm_model.pth`**: Pre-trained LSTM model\n",
    "\n",
    "These models are loaded in the `app.py` file for translation purposes. If you don't have these models, you can train them using the provided `train_transformer.py` and `train_lstm.py` scripts.\n",
    "\n",
    "---\n",
    "\n",
    "## **Running the App**\n",
    "\n",
    "### **1. Launching the Streamlit App**\n",
    "\n",
    "The core of this project is the **Streamlit interface** (`app.py`), which allows users to input English sentences and get their corresponding Urdu translations.\n",
    "\n",
    "To run the app:\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "This will open a web browser with the interface where you can:\n",
    "- Enter English text to get the Urdu translation.\n",
    "- Visualize the attention weights for the Transformer model.\n",
    "- Evaluate the models on the test set and display BLEU and ROUGE scores.\n",
    "\n",
    "### **2. Training the Models**\n",
    "\n",
    "If you want to train the **Transformer** or **LSTM** models from scratch, you can run the following scripts:\n",
    "\n",
    "- **Training the Transformer model**:\n",
    "\n",
    "  ```bash\n",
    "  python train_transformer.py\n",
    "  ```\n",
    "\n",
    "- **Training the LSTM model**:\n",
    "\n",
    "  ```bash\n",
    "  python train_lstm.py\n",
    "  ```\n",
    "\n",
    "This will train the models on the UMC005 English-Urdu dataset and save the trained models in the `saved_models/` directory.\n",
    "\n",
    "### **3. Evaluating the Models**\n",
    "\n",
    "You can evaluate the models' performance (on BLEU and ROUGE metrics) by running the `evaluate.py` script:\n",
    "\n",
    "```bash\n",
    "python evaluate.py\n",
    "```\n",
    "\n",
    "This will evaluate both the **Transformer** and **LSTM** models on the test dataset and print the BLEU and ROUGE scores.\n",
    "\n",
    "### **4. Visualizing Attention**\n",
    "\n",
    "In the **Streamlit interface** (`app.py`), you can click the **\"Visualize Attention\"** button to see the attention heatmap for the Transformer model. This shows which parts of the input sentence the model focused on while generating the translation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Evaluation Metrics**\n",
    "\n",
    "### **1. BLEU Score**\n",
    "\n",
    "The **BLEU (Bilingual Evaluation Understudy)** score measures the precision of n-grams (typically unigram, bigram, trigram) between the predicted and reference translations. A higher BLEU score indicates better translation quality.\n",
    "\n",
    "### **2. ROUGE Score**\n",
    "\n",
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** is another metric that evaluates the overlap between n-grams in the predicted and reference sentences. ROUGE-1, ROUGE-2, and ROUGE-L scores are commonly used to evaluate translation quality.\n",
    "\n",
    "Both **BLEU** and **ROUGE** scores are computed in the `evaluate.py` and displayed on the Streamlit interface.\n",
    "\n",
    "---\n",
    "\n",
    "## **Attention Visualization**\n",
    "\n",
    "The **Transformer model** includes an attention mechanism, which helps the model decide which parts of the input to focus on during translation. The attention weights can be visualized as a heatmap, showing the alignment between words in the input and output sentences. This is done using the **`VizAttention()`** function in the `utils/visualization.py` file.\n",
    "\n",
    "### Example of Attention Visualization:\n",
    "\n",
    "- **Input**: \"I love books.\"\n",
    "- **Output**: \"مجھے کتابیں پسند ہیں۔\"\n",
    "- The heatmap will show how much attention was given to each word in the input sentence when generating each word in the output sentence.\n",
    "\n",
    "---\n",
    "\n",
    "## **Future Improvements**\n",
    "\n",
    "### **1. Fine-tuning Pre-trained Models**\n",
    "Currently, the models are trained from scratch. However, fine-tuning **pre-trained models** (like **BERT** or **GPT**) on the English-Urdu dataset might improve performance. You could experiment with fine-tuning these models using **transfer learning**.\n",
    "\n",
    "### **2. Hyperparameter Tuning**\n",
    "Although hyperparameters like learning rate, batch size, and model depth are set, further **hyperparameter tuning** using techniques like **Grid Search** or **Bayesian Optimization** could yield better results.\n",
    "\n",
    "### **3. Multi-Lingual Models**\n",
    "This project focuses on **English to Urdu** translation. However, you could extend it to **multi-lingual translation** by training on additional language pairs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "This project demonstrates the implementation of machine translation from **English to Urdu** using both **Transformer** and **LSTM** models. It provides a **Streamlit-based user interface** for easy text input and translation, along with model evaluation and attention visualization. You can either use pre-trained models or train your own on the **UMC005 English-Urdu Parallel Corpus**.\n",
    "\n",
    "Feel free to experiment with the models, fine-tune them, and extend the project for additional language pairs and features!\n",
    "\n",
    "---\n",
    "\n",
    "### **Acknowledgments**\n",
    "- The project uses the **UMC005 English-Urdu Parallel Corpus** for training and evaluation.\n",
    "- The models are based on the Transformer architecture as described in the paper **\"Attention is All You Need\"** and the LSTM-based sequence-to-sequence model.\n",
    "\n",
    "---\n",
    "\n",
    "### **License**\n",
    "\n",
    "This project is open-source and free for non-commercial educational and research purposes. For commercial use, please reach out to the authors for licensing details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218594a",
   "metadata": {},
   "source": [
    "Keeping this and above objectives in mind write complete code:\n",
    "Input folder format:\n",
    "bible:\n",
    "Bible-EN\n",
    "Bible-UR\n",
    "Bible-UR-normalized\n",
    "dev.en\n",
    "dev.ur\n",
    "test.en\n",
    "test.ur\n",
    "train.en\n",
    "train.ur\n",
    "\n",
    "quran:\n",
    "Quran-EN\n",
    "Quran-UR\n",
    "Quran-UR-normalized\n",
    "dev.en\n",
    "dev.ur\n",
    "test.en\n",
    "test.ur\n",
    "train.en\n",
    "train.ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08e80c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# import math\n",
    "# import time\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import nltk\n",
    "# from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "# from rouge_score import rouge_scorer\n",
    "# import tkinter as tk\n",
    "# from tkinter import scrolledtext\n",
    "# import unicodedata\n",
    "# from torchtext.data.metrics import bleu_score\n",
    "# from collections import Counter, defaultdict\n",
    "\n",
    "# # Download NLTK resources\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # Set seeds for reproducibility\n",
    "# SEED = 42\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# random.seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# # Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Define hyperparameters\n",
    "# BATCH_SIZE = 64\n",
    "# EMBEDDING_SIZE = 512\n",
    "# NUM_HEADS = 8\n",
    "# NUM_ENCODER_LAYERS = 6\n",
    "# NUM_DECODER_LAYERS = 6\n",
    "# FFN_DIM = 2048\n",
    "# DROPOUT = 0.1\n",
    "# LEARNING_RATE = 0.0001\n",
    "# NUM_EPOCHS = 20\n",
    "# MAX_SEQ_LENGTH = 100\n",
    "# VOCAB_SIZE = 32000  # For BPE tokenization\n",
    "\n",
    "# # Data path configuration - adjust based on your file structure\n",
    "# DATA_DIR = \"data\"  # Base directory for the corpus\n",
    "# TRAIN_EN_PATH = os.path.join(DATA_DIR, \"bible/train.en\")\n",
    "# TRAIN_UR_PATH = os.path.join(DATA_DIR, \"bible/train.ur\")\n",
    "# DEV_EN_PATH = os.path.join(DATA_DIR, \"bible/dev.en\")\n",
    "# DEV_UR_PATH = os.path.join(DATA_DIR, \"bible/dev.ur\")\n",
    "# TEST_EN_PATH = os.path.join(DATA_DIR, \"bible/test.en\")\n",
    "# TEST_UR_PATH = os.path.join(DATA_DIR, \"bible/test.ur\")\n",
    "\n",
    "# # Add Quran data paths\n",
    "# QURAN_TRAIN_EN_PATH = os.path.join(DATA_DIR, \"quran/train.en\")\n",
    "# QURAN_TRAIN_UR_PATH = os.path.join(DATA_DIR, \"quran/train.ur\")\n",
    "# QURAN_DEV_EN_PATH = os.path.join(DATA_DIR, \"quran/dev.en\")\n",
    "# QURAN_DEV_UR_PATH = os.path.join(DATA_DIR, \"quran/dev.ur\")\n",
    "# QURAN_TEST_EN_PATH = os.path.join(DATA_DIR, \"quran/test.en\")\n",
    "# QURAN_TEST_UR_PATH = os.path.join(DATA_DIR, \"quran/test.ur\")\n",
    "\n",
    "# # Add model save paths\n",
    "# MODEL_SAVE_DIR = \"saved_models\"\n",
    "# TRANSFORMER_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"transformer_model.pt\")\n",
    "# LSTM_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"lstm_model.pt\")\n",
    "# TOKENIZER_EN_PATH = os.path.join(MODEL_SAVE_DIR, \"tokenizer_en.pt\")\n",
    "# TOKENIZER_UR_PATH = os.path.join(MODEL_SAVE_DIR, \"tokenizer_ur.pt\")\n",
    "\n",
    "# os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# # Part 1: Data Preprocessing and Tokenization\n",
    "# #############################################################\n",
    "\n",
    "# class BPETokenizer:\n",
    "#     \"\"\"\n",
    "#     Byte Pair Encoding (BPE) tokenizer for both English and Urdu text\n",
    "#     \"\"\"\n",
    "#     def __init__(self, vocab_size=VOCAB_SIZE):\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.special_tokens = {\n",
    "#             \"<PAD>\": 0,  # Padding token\n",
    "#             \"<SOS>\": 1,  # Start of sentence token\n",
    "#             \"<EOS>\": 2,  # End of sentence token\n",
    "#             \"<UNK>\": 3   # Unknown token\n",
    "#         }\n",
    "#         self.token_to_id = {token: idx for token, idx in self.special_tokens.items()}\n",
    "#         self.id_to_token = {idx: token for token, idx in self.special_tokens.items()}\n",
    "#         self.merges = {}\n",
    "#         self.vocab = {}\n",
    "        \n",
    "#     def train(self, texts):\n",
    "#         \"\"\"\n",
    "#         Train the BPE tokenizer on a list of texts\n",
    "#         \"\"\"\n",
    "#         # Initialize vocabulary with characters\n",
    "#         word_freqs = Counter()\n",
    "#         for text in texts:\n",
    "#             word_freqs.update(text.split())\n",
    "        \n",
    "#         # Initialize each word as a sequence of characters\n",
    "#         self.vocab = {token: list(token) for token in word_freqs.keys()}\n",
    "        \n",
    "#         # Count pairs\n",
    "#         pairs = self._count_pairs(self.vocab, word_freqs)\n",
    "        \n",
    "#         # Merge pairs until vocab_size is reached or no more pairs\n",
    "#         num_merges = self.vocab_size - len(self.special_tokens)\n",
    "#         for i in range(num_merges):\n",
    "#             if not pairs:\n",
    "#                 break\n",
    "                \n",
    "#             # Find most frequent pair\n",
    "#             best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "#             # Update vocabulary and merges\n",
    "#             self._merge_pair(best_pair, self.vocab, word_freqs)\n",
    "#             self.merges[best_pair] = len(self.token_to_id)\n",
    "            \n",
    "#             # Add new token to vocabulary\n",
    "#             new_token = best_pair[0] + best_pair[1]\n",
    "#             self.token_to_id[new_token] = len(self.token_to_id)\n",
    "#             self.id_to_token[self.token_to_id[new_token]] = new_token\n",
    "            \n",
    "#             # Update pair counts\n",
    "#             pairs = self._count_pairs(self.vocab, word_freqs)\n",
    "            \n",
    "#             if i % 1000 == 0:\n",
    "#                 print(f\"Merge {i}/{num_merges}: {best_pair} -> {new_token}\")\n",
    "    \n",
    "#     def _count_pairs(self, vocab, word_freqs):\n",
    "#         \"\"\"\n",
    "#         Count the frequency of adjacent pairs in the vocabulary\n",
    "#         \"\"\"\n",
    "#         pairs = defaultdict(int)\n",
    "#         for word, freq in word_freqs.items():\n",
    "#             word_tokens = vocab[word]\n",
    "#             for i in range(len(word_tokens) - 1):\n",
    "#                 pairs[(word_tokens[i], word_tokens[i+1])] += freq\n",
    "#         return pairs\n",
    "    \n",
    "#     def _merge_pair(self, pair, vocab, word_freqs):\n",
    "#         \"\"\"\n",
    "#         Merge a pair of tokens in the vocabulary\n",
    "#         \"\"\"\n",
    "#         for word in vocab:\n",
    "#             word_tokens = vocab[word]\n",
    "#             new_tokens = []\n",
    "#             i = 0\n",
    "#             while i < len(word_tokens):\n",
    "#                 if i < len(word_tokens) - 1 and (word_tokens[i], word_tokens[i+1]) == pair:\n",
    "#                     new_tokens.append(word_tokens[i] + word_tokens[i+1])\n",
    "#                     i += 2\n",
    "#                 else:\n",
    "#                     new_tokens.append(word_tokens[i])\n",
    "#                     i += 1\n",
    "#             vocab[word] = new_tokens\n",
    "    \n",
    "#     def tokenize(self, text):\n",
    "#         \"\"\"\n",
    "#         Tokenize a text using the learned BPE merges\n",
    "#         \"\"\"\n",
    "#         words = text.split()\n",
    "#         tokens = []\n",
    "        \n",
    "#         for word in words:\n",
    "#             # Start with characters\n",
    "#             word_tokens = list(word)\n",
    "            \n",
    "#             # Apply merges\n",
    "#             while len(word_tokens) > 1:\n",
    "#                 pairs = [(word_tokens[i], word_tokens[i+1]) for i in range(len(word_tokens)-1)]\n",
    "                \n",
    "#                 # Find the pair with the highest priority (first occurrence in merges)\n",
    "#                 pair_scores = [(pair, self.merges.get(pair, float('inf'))) for pair in pairs]\n",
    "#                 best_pair, best_score = min(pair_scores, key=lambda x: x[1])\n",
    "                \n",
    "#                 if best_pair not in self.merges:\n",
    "#                     break\n",
    "                    \n",
    "#                 # Apply the merge\n",
    "#                 new_tokens = []\n",
    "#                 i = 0\n",
    "#                 while i < len(word_tokens):\n",
    "#                     if i < len(word_tokens) - 1 and (word_tokens[i], word_tokens[i+1]) == best_pair:\n",
    "#                         new_tokens.append(word_tokens[i] + word_tokens[i+1])\n",
    "#                         i += 2\n",
    "#                     else:\n",
    "#                         new_tokens.append(word_tokens[i])\n",
    "#                         i += 1\n",
    "#                 word_tokens = new_tokens\n",
    "            \n",
    "#             tokens.extend(word_tokens)\n",
    "        \n",
    "#         # Add EOS token\n",
    "#         tokens.append(\"<EOS>\")\n",
    "        \n",
    "#         # Convert tokens to IDs\n",
    "#         ids = []\n",
    "#         for token in tokens:\n",
    "#             if token in self.token_to_id:\n",
    "#                 ids.append(self.token_to_id[token])\n",
    "#             else:\n",
    "#                 ids.append(self.special_tokens[\"<UNK>\"])\n",
    "        \n",
    "#         return ids\n",
    "    \n",
    "#     def detokenize(self, ids):\n",
    "#         \"\"\"\n",
    "#         Convert token IDs back to text\n",
    "#         \"\"\"\n",
    "#         tokens = [self.id_to_token.get(id, \"<UNK>\") for id in ids if id != self.special_tokens[\"<PAD>\"]]\n",
    "        \n",
    "#         # Remove special tokens\n",
    "#         tokens = [token for token in tokens if token not in [\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"]]\n",
    "        \n",
    "#         # Join tokens to form the text\n",
    "#         text = ''.join(tokens)\n",
    "        \n",
    "#         return text\n",
    "    \n",
    "#     def save(self, path):\n",
    "#         \"\"\"\n",
    "#         Save tokenizer to a file\n",
    "#         \"\"\"\n",
    "#         torch.save({\n",
    "#             'token_to_id': self.token_to_id,\n",
    "#             'id_to_token': self.id_to_token,\n",
    "#             'merges': self.merges,\n",
    "#             'vocab': self.vocab,\n",
    "#             'vocab_size': self.vocab_size\n",
    "#         }, path)\n",
    "    \n",
    "#     def load(self, path):\n",
    "#         \"\"\"\n",
    "#         Load tokenizer from a file\n",
    "#         \"\"\"\n",
    "#         data = torch.load(path)\n",
    "#         self.token_to_id = data['token_to_id']\n",
    "#         self.id_to_token = data['id_to_token']\n",
    "#         self.merges = data['merges']\n",
    "#         self.vocab = data['vocab']\n",
    "#         self.vocab_size = data['vocab_size']\n",
    "\n",
    "\n",
    "# def preprocess_english(text):\n",
    "#     \"\"\"\n",
    "#     Preprocess English text\n",
    "#     \"\"\"\n",
    "#     # Convert to lowercase\n",
    "#     text = text.lower()\n",
    "    \n",
    "#     # Add spaces around punctuation\n",
    "#     text = re.sub(r'([.,!?;:])', r' \\1 ', text)\n",
    "    \n",
    "#     # Remove multiple spaces\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# def preprocess_urdu(text):\n",
    "#     \"\"\"\n",
    "#     Preprocess Urdu text\n",
    "#     \"\"\"\n",
    "#     # Add spaces around punctuation\n",
    "#     text = re.sub(r'([۔،!?;:])', r' \\1 ', text)\n",
    "    \n",
    "#     # Normalize Unicode\n",
    "#     text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "#     # Remove multiple spaces\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# def load_corpus(en_path, ur_path):\n",
    "#     \"\"\"\n",
    "#     Load and preprocess English-Urdu corpus\n",
    "#     \"\"\"\n",
    "#     with open(en_path, 'r', encoding='utf-8') as f:\n",
    "#         en_lines = [preprocess_english(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "#     with open(ur_path, 'r', encoding='utf-8') as f:\n",
    "#         ur_lines = [preprocess_urdu(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "#     # Ensure equal length\n",
    "#     assert len(en_lines) == len(ur_lines), f\"Mismatch in corpus lengths: {len(en_lines)} vs {len(ur_lines)}\"\n",
    "    \n",
    "#     # Filter out empty lines and lines that are too long\n",
    "#     filtered_pairs = [(en, ur) for en, ur in zip(en_lines, ur_lines) \n",
    "#                      if len(en.split()) <= MAX_SEQ_LENGTH and len(ur.split()) <= MAX_SEQ_LENGTH]\n",
    "    \n",
    "#     en_filtered, ur_filtered = zip(*filtered_pairs) if filtered_pairs else ([], [])\n",
    "    \n",
    "#     return list(en_filtered), list(ur_filtered)\n",
    "\n",
    "# def train_tokenizers(en_texts, ur_texts):\n",
    "#     \"\"\"\n",
    "#     Train BPE tokenizers for English and Urdu\n",
    "#     \"\"\"\n",
    "#     # Initialize tokenizers\n",
    "#     en_tokenizer = BPETokenizer(vocab_size=VOCAB_SIZE)\n",
    "#     ur_tokenizer = BPETokenizer(vocab_size=VOCAB_SIZE)\n",
    "    \n",
    "#     # Train tokenizers\n",
    "#     print(\"Training English tokenizer...\")\n",
    "#     en_tokenizer.train(en_texts)\n",
    "    \n",
    "#     print(\"Training Urdu tokenizer...\")\n",
    "#     ur_tokenizer.train(ur_texts)\n",
    "    \n",
    "#     # Save tokenizers\n",
    "#     en_tokenizer.save(TOKENIZER_EN_PATH)\n",
    "#     ur_tokenizer.save(TOKENIZER_UR_PATH)\n",
    "    \n",
    "#     return en_tokenizer, ur_tokenizer\n",
    "\n",
    "# class TranslationDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Dataset for machine translation\n",
    "#     \"\"\"\n",
    "#     def __init__(self, en_texts, ur_texts, en_tokenizer, ur_tokenizer):\n",
    "#         self.en_texts = en_texts\n",
    "#         self.ur_texts = ur_texts\n",
    "#         self.en_tokenizer = en_tokenizer\n",
    "#         self.ur_tokenizer = ur_tokenizer\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.en_texts)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         en_text = self.en_texts[idx]\n",
    "#         ur_text = self.ur_texts[idx]\n",
    "        \n",
    "#         # Tokenize texts\n",
    "#         en_tokens = [self.en_tokenizer.special_tokens[\"<SOS>\"]] + self.en_tokenizer.tokenize(en_text)\n",
    "#         ur_tokens = [self.ur_tokenizer.special_tokens[\"<SOS>\"]] + self.ur_tokenizer.tokenize(ur_text)\n",
    "        \n",
    "#         return {\n",
    "#             'en_text': en_text,\n",
    "#             'ur_text': ur_text,\n",
    "#             'en_tokens': torch.tensor(en_tokens),\n",
    "#             'ur_tokens': torch.tensor(ur_tokens)\n",
    "#         }\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     Collate function for batching\n",
    "#     \"\"\"\n",
    "#     en_texts = [item['en_text'] for item in batch]\n",
    "#     ur_texts = [item['ur_text'] for item in batch]\n",
    "    \n",
    "#     en_tokens = [item['en_tokens'] for item in batch]\n",
    "#     ur_tokens = [item['ur_tokens'] for item in batch]\n",
    "    \n",
    "#     # Pad sequences\n",
    "#     en_tokens_padded = pad_sequence(en_tokens, batch_first=True, padding_value=0)\n",
    "#     ur_tokens_padded = pad_sequence(ur_tokens, batch_first=True, padding_value=0)\n",
    "    \n",
    "#     return {\n",
    "#         'en_texts': en_texts,\n",
    "#         'ur_texts': ur_texts,\n",
    "#         'en_tokens': en_tokens_padded,\n",
    "#         'ur_tokens': ur_tokens_padded\n",
    "#     }\n",
    "\n",
    "# # Part 2: Transformer Model Implementation\n",
    "# #############################################################\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Positional encoding for transformer model\n",
    "#     \"\"\"\n",
    "#     def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "#         # Create positional encoding\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         self.register_buffer('pe', pe)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "#         \"\"\"\n",
    "#         x = x + self.pe[:x.size(0), :]\n",
    "#         return self.dropout(x)\n",
    "\n",
    "# class Transformer(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Transformer model for machine translation\n",
    "#     \"\"\"\n",
    "#     def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers,\n",
    "#                  num_decoder_layers, dim_feedforward, dropout=0.1):\n",
    "#         super(Transformer, self).__init__()\n",
    "        \n",
    "#         # Embedding layers\n",
    "#         self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "#         self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "#         # Positional encoding\n",
    "#         self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "#         # Transformer layers\n",
    "#         self.transformer = nn.Transformer(\n",
    "#             d_model=d_model,\n",
    "#             nhead=nhead,\n",
    "#             num_encoder_layers=num_encoder_layers,\n",
    "#             num_decoder_layers=num_decoder_layers,\n",
    "#             dim_feedforward=dim_feedforward,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "        \n",
    "#         # Output layer\n",
    "#         self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "#         # Initialize parameters\n",
    "#         self._init_parameters()\n",
    "        \n",
    "#     def _init_parameters(self):\n",
    "#         \"\"\"\n",
    "#         Initialize parameters with Xavier uniform distribution\n",
    "#         \"\"\"\n",
    "#         for p in self.parameters():\n",
    "#             if p.dim() > 1:\n",
    "#                 nn.init.xavier_uniform_(p)\n",
    "    \n",
    "#     def forward(self, src, tgt, src_mask=None, tgt_mask=None, \n",
    "#                 memory_mask=None, src_key_padding_mask=None, \n",
    "#                 tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             src: Source sequence [batch_size, src_len]\n",
    "#             tgt: Target sequence [batch_size, tgt_len]\n",
    "#             *_mask, *_key_padding_mask: Masks for attention mechanism\n",
    "#         \"\"\"\n",
    "#         # Create padding masks\n",
    "#         if src_key_padding_mask is None:\n",
    "#             src_key_padding_mask = (src == 0)\n",
    "        \n",
    "#         if tgt_key_padding_mask is None:\n",
    "#             tgt_key_padding_mask = (tgt == 0)\n",
    "        \n",
    "#         # Create causal mask for decoder\n",
    "#         if tgt_mask is None:\n",
    "#             tgt_len = tgt.size(1)\n",
    "#             tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_len).to(tgt.device)\n",
    "        \n",
    "#         # Embed tokens\n",
    "#         src_embedded = self.src_embedding(src)\n",
    "#         tgt_embedded = self.tgt_embedding(tgt)\n",
    "        \n",
    "#         # Add positional encoding\n",
    "#         src_embedded = self.positional_encoding(src_embedded)\n",
    "#         tgt_embedded = self.positional_encoding(tgt_embedded)\n",
    "        \n",
    "#         # Forward pass through transformer\n",
    "#         output = self.transformer(\n",
    "#             src=src_embedded,\n",
    "#             tgt=tgt_embedded,\n",
    "#             src_mask=src_mask,\n",
    "#             tgt_mask=tgt_mask,\n",
    "#             memory_mask=memory_mask,\n",
    "#             src_key_padding_mask=src_key_padding_mask,\n",
    "#             tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "#             memory_key_padding_mask=src_key_padding_mask\n",
    "#         )\n",
    "        \n",
    "#         # Project to vocabulary\n",
    "#         output = self.fc_out(output)\n",
    "        \n",
    "#         return output\n",
    "    \n",
    "#     def get_attention_weights(self, src, tgt, src_mask=None, tgt_mask=None, \n",
    "#                               memory_mask=None, src_key_padding_mask=None, \n",
    "#                               tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "#         \"\"\"\n",
    "#         Retrieve attention weights for visualization\n",
    "#         \"\"\"\n",
    "#         # Create padding masks\n",
    "#         if src_key_padding_mask is None:\n",
    "#             src_key_padding_mask = (src == 0)\n",
    "        \n",
    "#         if tgt_key_padding_mask is None:\n",
    "#             tgt_key_padding_mask = (tgt == 0)\n",
    "        \n",
    "#         # Create causal mask for decoder\n",
    "#         if tgt_mask is None:\n",
    "#             tgt_len = tgt.size(1)\n",
    "#             tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_len).to(tgt.device)\n",
    "        \n",
    "#         # Embed tokens\n",
    "#         src_embedded = self.src_embedding(src)\n",
    "#         tgt_embedded = self.tgt_embedding(tgt)\n",
    "        \n",
    "#         # Add positional encoding\n",
    "#         src_embedded = self.positional_encoding(src_embedded)\n",
    "#         tgt_embedded = self.positional_encoding(tgt_embedded)\n",
    "        \n",
    "#         # Store attention weights\n",
    "#         attention_weights = {}\n",
    "        \n",
    "#         # Encoder self-attention\n",
    "#         for i, layer in enumerate(self.transformer.encoder.layers):\n",
    "#             # Hook to capture attention weights\n",
    "#             def get_attention_hook(layer_idx):\n",
    "#                 def hook(module, input, output):\n",
    "#                     attention_weights[f'encoder_layer_{layer_idx}'] = module.self_attn.attn_output_weights\n",
    "#                 return hook\n",
    "            \n",
    "#             handle = layer.self_attn.register_forward_hook(get_attention_hook(i))\n",
    "            \n",
    "#         # Decoder self-attention and cross-attention\n",
    "#         for i, layer in enumerate(self.transformer.decoder.layers):\n",
    "#             # Hook for self-attention\n",
    "#             def get_self_attention_hook(layer_idx):\n",
    "#                 def hook(module, input, output):\n",
    "#                     attention_weights[f'decoder_self_attn_layer_{layer_idx}'] = module.self_attn.attn_output_weights\n",
    "#                 return hook\n",
    "            \n",
    "#             # Hook for cross-attention\n",
    "#             def get_cross_attention_hook(layer_idx):\n",
    "#                 def hook(module, input, output):\n",
    "#                     attention_weights[f'decoder_cross_attn_layer_{layer_idx}'] = module.multihead_attn.attn_output_weights\n",
    "#                 return hook\n",
    "            \n",
    "#             handle_self = layer.self_attn.register_forward_hook(get_self_attention_hook(i))\n",
    "#             handle_cross = layer.multihead_attn.register_forward_hook(get_cross_attention_hook(i))\n",
    "        \n",
    "#         # Forward pass through transformer\n",
    "#         output = self.transformer(\n",
    "#             src=src_embedded,\n",
    "#             tgt=tgt_embedded,\n",
    "#             src_mask=src_mask,\n",
    "#             tgt_mask=tgt_mask,\n",
    "#             memory_mask=memory_mask,\n",
    "#             src_key_padding_mask=src_key_padding_mask,\n",
    "#             tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "#             memory_key_padding_mask=src_key_padding_mask\n",
    "#         )\n",
    "        \n",
    "#         # Remove hooks\n",
    "#         for handle in [handle] + [handle_self, handle_cross]:\n",
    "#             handle.remove()\n",
    "        \n",
    "#         return attention_weights\n",
    "\n",
    "# # LSTM Model Implementation\n",
    "# class Encoder(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Bidirectional Encoder for LSTM model\n",
    "#     \"\"\"\n",
    "#     def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.hid_dim = hid_dim\n",
    "#         self.n_layers = n_layers\n",
    "        \n",
    "#         self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "#         self.rnn = nn.LSTM(\n",
    "#             emb_dim,\n",
    "#             hid_dim,\n",
    "#             n_layers,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True   # ✅ now bidirectional\n",
    "#         )\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, src):\n",
    "#         \"\"\"\n",
    "#         src: [batch_size, src_len]\n",
    "#         returns:\n",
    "#           - outputs: [batch_size, src_len, hid_dim * 2]  # ✅ (forward + backward)\n",
    "#           - hidden: [n_layers * 2, batch_size, hid_dim]\n",
    "#           - cell:   [n_layers * 2, batch_size, hid_dim]\n",
    "#         \"\"\"\n",
    "#         embedded = self.dropout(self.embedding(src))\n",
    "#         outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "#         return outputs, hidden, cell\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Attention mechanism for Bidirectional Encoder\n",
    "#     \"\"\"\n",
    "#     def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)  # ✅ enc_hid_dim * 2\n",
    "#         self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "        \n",
    "#     def forward(self, hidden, encoder_outputs, mask=None):\n",
    "#         \"\"\"\n",
    "#         hidden: [batch_size, dec_hid_dim]\n",
    "#         encoder_outputs: [batch_size, src_len, enc_hid_dim * 2]\n",
    "#         mask: [batch_size, src_len] (optional)\n",
    "#         \"\"\"\n",
    "#         batch_size = encoder_outputs.shape[0]\n",
    "#         src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "#         # Repeat hidden for each time step\n",
    "#         hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "#         # Calculate energy\n",
    "#         energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        \n",
    "#         # Calculate attention scores\n",
    "#         attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "#         # Apply mask if given\n",
    "#         if mask is not None:\n",
    "#             attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "#         attention_weights = torch.softmax(attention, dim=1)\n",
    "        \n",
    "#         # Calculate context vector\n",
    "#         context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "#         return context, attention_weights\n",
    "\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.output_dim = output_dim\n",
    "#         self.attention = attention\n",
    "\n",
    "#         self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "#         self.rnn = nn.LSTM(\n",
    "#             (enc_hid_dim * 2) + emb_dim,  # Update to match encoder's bidirectional output size\n",
    "#             dec_hid_dim,\n",
    "#             n_layers,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "\n",
    "#         self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)  # Update input size\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, input, hidden, cell, encoder_outputs, mask=None):\n",
    "#         embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "#         context, attention_weights = self.attention(hidden[-1], encoder_outputs, mask)\n",
    "\n",
    "#         rnn_input = torch.cat((embedded, context.unsqueeze(1)), dim=2)\n",
    "\n",
    "#         output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "\n",
    "#         output = torch.cat((output.squeeze(1), context, embedded.squeeze(1)), dim=1)\n",
    "\n",
    "#         prediction = self.fc_out(output)\n",
    "\n",
    "#         return prediction, hidden, cell, attention_weights\n",
    "\n",
    "# class LSTMSeq2Seq(nn.Module):\n",
    "#     def __init__(self, encoder, decoder, device):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.device = device\n",
    "\n",
    "#     def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "#         batch_size = src.shape[0]\n",
    "#         tgt_len = tgt.shape[1]\n",
    "#         tgt_vocab_size = self.decoder.output_dim\n",
    "\n",
    "#         outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "\n",
    "#         src_mask = (src != 0).float().to(self.device)\n",
    "\n",
    "#         # Encode source sentence\n",
    "#         encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "#         # Merge forward and backward hidden and cell states\n",
    "#         hidden = self._cat_directions(hidden)\n",
    "#         cell = self._cat_directions(cell)\n",
    "\n",
    "#         # First input to the decoder is the <SOS> token\n",
    "#         input = tgt[:, 0].unsqueeze(1)\n",
    "\n",
    "#         attention_weights = []\n",
    "\n",
    "#         for t in range(1, tgt_len):\n",
    "#             output, hidden, cell, attn_weights = self.decoder(input, hidden, cell, encoder_outputs, src_mask)\n",
    "#             outputs[:, t, :] = output\n",
    "#             attention_weights.append(attn_weights)\n",
    "\n",
    "#             teacher_force = random.random() < teacher_forcing_ratio\n",
    "#             top1 = output.argmax(1).unsqueeze(1)\n",
    "#             input = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
    "\n",
    "#         return outputs, torch.stack(attention_weights, dim=1)\n",
    "\n",
    "#     def _cat_directions(self, h):\n",
    "#         new_h = torch.cat((h[0:h.size(0):2], h[1:h.size(0):2]), dim=2)  # Concatenate forward and backward\n",
    "#         return new_h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb77583",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9eabf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import unicodedata\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 512\n",
    "NUM_HEADS = 8\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "FFN_DIM = 2048\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 20\n",
    "MAX_SEQ_LENGTH = 100\n",
    "VOCAB_SIZE = 32000  # For BPE tokenization\n",
    "\n",
    "# Data path configuration - adjust based on your file structure\n",
    "DATA_DIR = \"data\"  # Base directory for the corpus\n",
    "TRAIN_EN_PATH = os.path.join(DATA_DIR, \"bible/train.en\")\n",
    "TRAIN_UR_PATH = os.path.join(DATA_DIR, \"bible/train.ur\")\n",
    "DEV_EN_PATH = os.path.join(DATA_DIR, \"bible/dev.en\")\n",
    "DEV_UR_PATH = os.path.join(DATA_DIR, \"bible/dev.ur\")\n",
    "TEST_EN_PATH = os.path.join(DATA_DIR, \"bible/test.en\")\n",
    "TEST_UR_PATH = os.path.join(DATA_DIR, \"bible/test.ur\")\n",
    "\n",
    "# Add Quran data paths\n",
    "QURAN_TRAIN_EN_PATH = os.path.join(DATA_DIR, \"quran/train.en\")\n",
    "QURAN_TRAIN_UR_PATH = os.path.join(DATA_DIR, \"quran/train.ur\")\n",
    "QURAN_DEV_EN_PATH = os.path.join(DATA_DIR, \"quran/dev.en\")\n",
    "QURAN_DEV_UR_PATH = os.path.join(DATA_DIR, \"quran/dev.ur\")\n",
    "QURAN_TEST_EN_PATH = os.path.join(DATA_DIR, \"quran/test.en\")\n",
    "QURAN_TEST_UR_PATH = os.path.join(DATA_DIR, \"quran/test.ur\")\n",
    "\n",
    "# Add model save paths\n",
    "MODEL_SAVE_DIR = \"saved_models\"\n",
    "TRANSFORMER_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"transformer_model.pt\")\n",
    "LSTM_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"lstm_model.pt\")\n",
    "TOKENIZER_EN_PATH = os.path.join(MODEL_SAVE_DIR, \"tokenizer_en.pt\")\n",
    "TOKENIZER_UR_PATH = os.path.join(MODEL_SAVE_DIR, \"tokenizer_ur.pt\")\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Part 1: Data Preprocessing and Tokenization\n",
    "#############################################################\n",
    "\n",
    "# Tokenizer and Preprocessing (same as in the original code, no changes needed)\n",
    "\n",
    "# Part 2: Transformer Model Implementation\n",
    "#############################################################\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding for transformer model\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model for machine translation\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers,\n",
    "                 num_decoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "    def _init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters with Xavier uniform distribution\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, \n",
    "                memory_mask=None, src_key_padding_mask=None, \n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Source sequence [batch_size, src_len]\n",
    "            tgt: Target sequence [batch_size, tgt_len]\n",
    "            *_mask, *_key_padding_mask: Masks for attention mechanism\n",
    "        \"\"\"\n",
    "        # Create padding masks\n",
    "        if src_key_padding_mask is None:\n",
    "            src_key_padding_mask = (src == 0)\n",
    "        \n",
    "        if tgt_key_padding_mask is None:\n",
    "            tgt_key_padding_mask = (tgt == 0)\n",
    "        \n",
    "        # Create causal mask for decoder\n",
    "        if tgt_mask is None:\n",
    "            tgt_len = tgt.size(1)\n",
    "            tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_len).to(tgt.device)\n",
    "        \n",
    "        # Embed tokens\n",
    "        src_embedded = self.src_embedding(src)\n",
    "        tgt_embedded = self.tgt_embedding(tgt)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        src_embedded = self.positional_encoding(src_embedded)\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded)\n",
    "        \n",
    "        # Forward pass through transformer\n",
    "        output = self.transformer(\n",
    "            src=src_embedded,\n",
    "            tgt=tgt_embedded,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_attention_weights(self, src, tgt, src_mask=None, tgt_mask=None, \n",
    "                              memory_mask=None, src_key_padding_mask=None, \n",
    "                              tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Retrieve attention weights for visualization\n",
    "        \"\"\"\n",
    "        # Create padding masks\n",
    "        if src_key_padding_mask is None:\n",
    "            src_key_padding_mask = (src == 0)\n",
    "        \n",
    "        if tgt_key_padding_mask is None:\n",
    "            tgt_key_padding_mask = (tgt == 0)\n",
    "        \n",
    "        # Create causal mask for decoder\n",
    "        if tgt_mask is None:\n",
    "            tgt_len = tgt.size(1)\n",
    "            tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_len).to(tgt.device)\n",
    "        \n",
    "        # Embed tokens\n",
    "        src_embedded = self.src_embedding(src)\n",
    "        tgt_embedded = self.tgt_embedding(tgt)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        src_embedded = self.positional_encoding(src_embedded)\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded)\n",
    "        \n",
    "        # Store attention weights\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # Encoder self-attention\n",
    "        for i, layer in enumerate(self.transformer.encoder.layers):\n",
    "            # Hook to capture attention weights\n",
    "            def get_attention_hook(layer_idx):\n",
    "                def hook(module, input, output):\n",
    "                    attention_weights[f'encoder_layer_{layer_idx}'] = module.self_attn.attn_output_weights\n",
    "                return hook\n",
    "            \n",
    "            handle = layer.self_attn.register_forward_hook(get_attention_hook(i))\n",
    "            \n",
    "        # Decoder self-attention and cross-attention\n",
    "        for i, layer in enumerate(self.transformer.decoder.layers):\n",
    "            # Hook for self-attention\n",
    "            def get_self_attention_hook(layer_idx):\n",
    "                def hook(module, input, output):\n",
    "                    attention_weights[f'decoder_self_attn_layer_{layer_idx}'] = module.self_attn.attn_output_weights\n",
    "                return hook\n",
    "            \n",
    "            # Hook for cross-attention\n",
    "            def get_cross_attention_hook(layer_idx):\n",
    "                def hook(module, input, output):\n",
    "                    attention_weights[f'decoder_cross_attn_layer_{layer_idx}'] = module.multihead_attn.attn_output_weights\n",
    "                return hook\n",
    "            \n",
    "            handle_self = layer.self_attn.register_forward_hook(get_self_attention_hook(i))\n",
    "            handle_cross = layer.multihead_attn.register_forward_hook(get_cross_attention_hook(i))\n",
    "        \n",
    "        # Forward pass through transformer\n",
    "        output = self.transformer(\n",
    "            src=src_embedded,\n",
    "            tgt=tgt_embedded,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Remove hooks\n",
    "        for handle in [handle] + [handle_self, handle_cross]:\n",
    "            handle.remove()\n",
    "        \n",
    "        return attention_weights\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism for Bidirectional Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Fix: Use bidirectional hidden size for encoder\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        hidden: [batch_size, dec_hid_dim]\n",
    "        encoder_outputs: [batch_size, src_len, enc_hid_dim * 2]\n",
    "        mask: [batch_size, src_len] (optional)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "\n",
    "        # Repeat hidden for each time step to match encoder outputs\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, dec_hid_dim]\n",
    "        \n",
    "        # Concatenate hidden state and encoder outputs\n",
    "        combined = torch.cat((hidden, encoder_outputs), dim=2)  # [batch_size, src_len, dec_hid_dim + enc_hid_dim * 2]\n",
    "        \n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(combined))  # [batch_size, src_len, dec_hid_dim]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
    "        \n",
    "        # Apply mask if given\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention_weights = torch.softmax(attention, dim=1)  # [batch_size, src_len]\n",
    "        \n",
    "        # Calculate context vector\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # [batch_size, enc_hid_dim * 2]\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "\n",
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "\n",
    "        src_mask = (src != 0).float().to(self.device)\n",
    "\n",
    "        # Encode source sentence\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        # Merge forward and backward hidden and cell states\n",
    "        hidden = self._cat_directions(hidden)\n",
    "        cell = self._cat_directions(cell)\n",
    "\n",
    "        # First input to the decoder is the <SOS> token\n",
    "        input = tgt[:, 0].unsqueeze(1)\n",
    "\n",
    "        attention_weights = []\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell, attn_weights = self.decoder(input, hidden, cell, encoder_outputs, src_mask)\n",
    "            outputs[:, t, :] = output\n",
    "            attention_weights.append(attn_weights)\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
    "\n",
    "        return outputs, torch.stack(attention_weights, dim=1)\n",
    "\n",
    "    def _cat_directions(self, h):\n",
    "        new_h = torch.cat((h[0:h.size(0):2], h[1:h.size(0):2]), dim=2)  # Concatenate forward and backward\n",
    "        return new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f043a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 754\u001b[0m\n\u001b[0;32m    751\u001b[0m     gui\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 754\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 695\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    692\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(MODEL_SAVE_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m--> 695\u001b[0m train_loader, dev_loader, test_loader, test_pairs, en_tokenizer, ur_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;66;03m# Check if models exist\u001b[39;00m\n\u001b[0;32m    698\u001b[0m transformer_exists \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(TRANSFORMER_MODEL_PATH)\n",
      "Cell \u001b[1;32mIn[3], line 314\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading and preprocessing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Load Bible corpus\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m en_bible_train, ur_bible_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_corpus\u001b[49m(TRAIN_EN_PATH, TRAIN_UR_PATH)\n\u001b[0;32m    315\u001b[0m en_bible_dev, ur_bible_dev \u001b[38;5;241m=\u001b[39m load_corpus(DEV_EN_PATH, DEV_UR_PATH)\n\u001b[0;32m    316\u001b[0m en_bible_test, ur_bible_test \u001b[38;5;241m=\u001b[39m load_corpus(TEST_EN_PATH, TEST_UR_PATH)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, clip, device, teacher_forcing_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Get input and target sequences\n",
    "        src = batch['en_tokens'].to(device)\n",
    "        tgt = batch['ur_tokens'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        if isinstance(model, Transformer):\n",
    "            # For transformer model\n",
    "            # Shift target for teacher forcing\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt_input)\n",
    "            \n",
    "            # Reshape output and target for loss calculation\n",
    "            output_flat = output.contiguous().view(-1, output.shape[-1])\n",
    "            tgt_output_flat = tgt_output.contiguous().view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_flat, tgt_output_flat)\n",
    "        else:\n",
    "            # For LSTM model\n",
    "            output, _ = model(src, tgt, teacher_forcing_ratio)\n",
    "            \n",
    "            # Reshape output and target for loss calculation\n",
    "            output_flat = output[:, 1:].contiguous().view(-1, output.shape[-1])\n",
    "            tgt_output_flat = tgt[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_flat, tgt_output_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Get input and target sequences\n",
    "            src = batch['en_tokens'].to(device)\n",
    "            tgt = batch['ur_tokens'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            if isinstance(model, Transformer):\n",
    "                # For transformer model\n",
    "                # Shift target for teacher forcing\n",
    "                tgt_input = tgt[:, :-1]\n",
    "                tgt_output = tgt[:, 1:]\n",
    "                \n",
    "                # Forward pass\n",
    "                output = model(src, tgt_input)\n",
    "                \n",
    "                # Reshape output and target for loss calculation\n",
    "                output_flat = output.contiguous().view(-1, output.shape[-1])\n",
    "                tgt_output_flat = tgt_output.contiguous().view(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(output_flat, tgt_output_flat)\n",
    "            else:\n",
    "                # For LSTM model\n",
    "                output, _ = model(src, tgt, 0)  # No teacher forcing during evaluation\n",
    "                \n",
    "                # Reshape output and target for loss calculation\n",
    "                output_flat = output[:, 1:].contiguous().view(-1, output.shape[-1])\n",
    "                tgt_output_flat = tgt[:, 1:].contiguous().view(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(output_flat, tgt_output_flat)\n",
    "            \n",
    "            # Update epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def translate_sentence(model, sentence, en_tokenizer, ur_tokenizer, device, max_length=MAX_SEQ_LENGTH):\n",
    "    \"\"\"\n",
    "    Translate a single English sentence to Urdu\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess and tokenize sentence\n",
    "    processed_sentence = preprocess_english(sentence)\n",
    "    tokens = [en_tokenizer.special_tokens[\"< SOS >\"]] + en_tokenizer.tokenize(processed_sentence)\n",
    "    tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize target with SOS token\n",
    "    if isinstance(model, Transformer):\n",
    "        # For transformer model\n",
    "        # Initialize target tensor with SOS token\n",
    "        tgt = torch.tensor([ur_tokenizer.special_tokens[\"< SOS >\"]]).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate translation one token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass\n",
    "            output = model(tokens_tensor, tgt)\n",
    "            \n",
    "            # Get the next token\n",
    "            next_token = output[:, -1, :].argmax(dim=1).item()\n",
    "            \n",
    "            # Break if EOS token\n",
    "            if next_token == ur_tokenizer.special_tokens[\"<EOS>\"]:\n",
    "                break\n",
    "            \n",
    "            # Add token to target\n",
    "            tgt = torch.cat([tgt, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "        \n",
    "        # Convert to list and remove SOS token\n",
    "        tgt = tgt.squeeze(0).tolist()[1:]\n",
    "    else:\n",
    "        # For LSTM model\n",
    "        # Encode source sentence\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, hidden, cell = model.encoder(tokens_tensor)\n",
    "        \n",
    "        # Initialize target with SOS token\n",
    "        tgt = [ur_tokenizer.special_tokens[\"< SOS >\"]]\n",
    "        \n",
    "        # Initialize hidden states\n",
    "        for _ in range(max_length):\n",
    "            # Convert token to tensor\n",
    "            tgt_tensor = torch.tensor([tgt[-1]]).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                output, hidden, cell, _ = model.decoder(tgt_tensor, hidden, cell, encoder_outputs)\n",
    "            \n",
    "            # Get the next token\n",
    "            next_token = output.argmax(1).item()\n",
    "            \n",
    "            # Add token to target\n",
    "            tgt.append(next_token)\n",
    "            \n",
    "            # Break if EOS token\n",
    "            if next_token == ur_tokenizer.special_tokens[\"<EOS>\"]:\n",
    "                break\n",
    "        \n",
    "        # Remove SOS token\n",
    "        tgt = tgt[1:]\n",
    "    \n",
    "    # Convert tokens to text\n",
    "    translated_text = ur_tokenizer.detokenize(tgt)\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "def calculate_bleu(translation_pairs, en_tokenizer, ur_tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score\n",
    "    \"\"\"\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for en_text, ur_text in translation_pairs:\n",
    "        # Translate English text\n",
    "        translation = translate_sentence(model, en_text, en_tokenizer, ur_tokenizer, device)\n",
    "        \n",
    "        # Tokenize reference and hypothesis\n",
    "        reference_tokens = ur_text.split()\n",
    "        hypothesis_tokens = translation.split()\n",
    "        \n",
    "        # Add to lists\n",
    "        references.append([reference_tokens])\n",
    "        hypotheses.append(hypothesis_tokens)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothing)\n",
    "    \n",
    "    return bleu_score\n",
    "\n",
    "def calculate_rouge(translation_pairs, en_tokenizer, ur_tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE score\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "    rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "    \n",
    "    for en_text, ur_text in translation_pairs:\n",
    "        # Translate English text\n",
    "        translation = translate_sentence(model, en_text, en_tokenizer, ur_tokenizer, device)\n",
    "        \n",
    "        # Calculate ROUGE score\n",
    "        scores = scorer.score(ur_text, translation)\n",
    "        \n",
    "        # Update scores\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key] += scores[key].fmeasure\n",
    "    \n",
    "    # Calculate average\n",
    "    for key in rouge_scores:\n",
    "        rouge_scores[key] /= len(translation_pairs)\n",
    "    \n",
    "    return rouge_scores\n",
    "\n",
    "def plot_learning_curves(train_losses, valid_losses, save_path):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss curves\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def visualize_attention(model, sentence, en_tokenizer, ur_tokenizer, device):\n",
    "    \"\"\"\n",
    "    Visualize attention weights\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess and tokenize sentence\n",
    "    processed_sentence = preprocess_english(sentence)\n",
    "    tokens = [en_tokenizer.special_tokens[\"< SOS >\"]] + en_tokenizer.tokenize(processed_sentence)\n",
    "    tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # For transformer model\n",
    "    if isinstance(model, Transformer):\n",
    "        # Initialize target tensor with SOS token\n",
    "        tgt = torch.tensor([ur_tokenizer.special_tokens[\"< SOS >\"]]).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate translation one token at a time\n",
    "        attention_matrices = []\n",
    "        translated_tokens = []\n",
    "        \n",
    "        for _ in range(MAX_SEQ_LENGTH):\n",
    "            # Get attention weights\n",
    "            attention_weights = model.get_attention_weights(tokens_tensor, tgt)\n",
    "            \n",
    "            # Get cross-attention from the last decoder layer\n",
    "            cross_attention = attention_weights[f'decoder_cross_attn_layer_{NUM_DECODER_LAYERS-1}'].detach().cpu().numpy()\n",
    "            attention_matrices.append(cross_attention)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(tokens_tensor, tgt)\n",
    "            \n",
    "            # Get the next token\n",
    "            next_token = output[:, -1, :].argmax(dim=1).item()\n",
    "            translated_tokens.append(next_token)\n",
    "            \n",
    "            # Break if EOS token\n",
    "            if next_token == ur_tokenizer.special_tokens[\"<EOS>\"]:\n",
    "                break\n",
    "            \n",
    "            # Add token to target\n",
    "            tgt = torch.cat([tgt, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "        \n",
    "        # Convert tokens to text\n",
    "        en_text = processed_sentence.split()\n",
    "        ur_text = [ur_tokenizer.id_to_token.get(token, \"<UNK>\") for token in translated_tokens]\n",
    "        \n",
    "        # Plot attention heatmap\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        attention = np.mean(np.array(attention_matrices), axis=0).squeeze()\n",
    "        \n",
    "        # Ensure attention has the right shape\n",
    "        if len(attention.shape) == 3:\n",
    "            attention = attention[0]\n",
    "        \n",
    "        ax.imshow(attention[:len(ur_text), :len(en_text)], cmap='viridis')\n",
    "        \n",
    "        # Set axis labels\n",
    "        ax.set_xticks(range(len(en_text)))\n",
    "        ax.set_yticks(range(len(ur_text)))\n",
    "        ax.set_xticklabels(en_text, rotation=90)\n",
    "        ax.set_yticklabels(ur_text)\n",
    "        \n",
    "        # Set title\n",
    "        ax.set_title('Attention Visualization')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    else:\n",
    "        # For LSTM model (if needed)\n",
    "        return None\n",
    "\n",
    "# Part 4: Main Training Loop and GUI Implementation\n",
    "#############################################################\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess the corpus data\n",
    "    \"\"\"\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load Bible corpus\n",
    "    en_bible_train, ur_bible_train = load_corpus(TRAIN_EN_PATH, TRAIN_UR_PATH)\n",
    "    en_bible_dev, ur_bible_dev = load_corpus(DEV_EN_PATH, DEV_UR_PATH)\n",
    "    en_bible_test, ur_bible_test = load_corpus(TEST_EN_PATH, TEST_UR_PATH)\n",
    "    \n",
    "    # Load Quran corpus\n",
    "    en_quran_train, ur_quran_train = load_corpus(QURAN_TRAIN_EN_PATH, QURAN_TRAIN_UR_PATH)\n",
    "    en_quran_dev, ur_quran_dev = load_corpus(QURAN_DEV_EN_PATH, QURAN_DEV_UR_PATH)\n",
    "    en_quran_test, ur_quran_test = load_corpus(QURAN_TEST_EN_PATH, QURAN_TEST_UR_PATH)\n",
    "    \n",
    "    # Combine corpus\n",
    "    en_train = en_bible_train + en_quran_train\n",
    "    ur_train = ur_bible_train + ur_quran_train\n",
    "    en_dev = en_bible_dev + en_quran_dev\n",
    "    ur_dev = ur_bible_dev + ur_quran_dev\n",
    "    en_test = en_bible_test + en_quran_test\n",
    "    ur_test = ur_bible_test + ur_quran_test\n",
    "    \n",
    "    print(f\"Training set size: {len(en_train)}\")\n",
    "    print(f\"Validation set size: {len(en_dev)}\")\n",
    "    print(f\"Test set size: {len(en_test)}\")\n",
    "    \n",
    "    # Train tokenizers\n",
    "    en_tokenizer, ur_tokenizer = train_tokenizers(en_train, ur_train)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TranslationDataset(en_train, ur_train, en_tokenizer, ur_tokenizer)\n",
    "    dev_dataset = TranslationDataset(en_dev, ur_dev, en_tokenizer, ur_tokenizer)\n",
    "    test_dataset = TranslationDataset(en_test, ur_test, en_tokenizer, ur_tokenizer)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    # Create test pairs for BLEU and ROUGE calculations\n",
    "    test_pairs = list(zip(en_test, ur_test))\n",
    "    \n",
    "    return train_loader, dev_loader, test_loader, test_pairs, en_tokenizer, ur_tokenizer\n",
    "\n",
    "def train_transformer_model(train_loader, dev_loader, en_tokenizer, ur_tokenizer):\n",
    "    \"\"\"\n",
    "    Train the transformer model\n",
    "    \"\"\"\n",
    "    print(\"Training transformer model...\")\n",
    "    \n",
    "    # Initialize model\n",
    "    src_vocab_size = len(en_tokenizer.token_to_id)\n",
    "    tgt_vocab_size = len(ur_tokenizer.token_to_id)\n",
    "    \n",
    "    transformer = Transformer(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        d_model=EMBEDDING_SIZE,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        dim_feedforward=FFN_DIM,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = optim.Adam(transformer.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    # Initialize criterion\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_valid_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Initialize lists for learning curves\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(transformer, train_loader, optimizer, criterion, 1.0, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        valid_loss = evaluate(transformer, dev_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(valid_loss)\n",
    "        \n",
    "        # Save losses for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        # Calculate time elapsed\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.4f} | Train PPL: {math.exp(train_loss):.4f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. PPL: {math.exp(valid_loss):.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(transformer.state_dict(), TRANSFORMER_MODEL_PATH)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "    \n",
    "    # Plot learning curves\n",
    "    plot_learning_curves(train_losses, valid_losses, 'transformer_learning_curves.png')\n",
    "    \n",
    "    # Load best model\n",
    "    transformer.load_state_dict(torch.load(TRANSFORMER_MODEL_PATH))\n",
    "    \n",
    "    return transformer\n",
    "\n",
    "def train_lstm_model(train_loader, dev_loader, en_tokenizer, ur_tokenizer):\n",
    "    \"\"\"\n",
    "    Train the LSTM model\n",
    "    \"\"\"\n",
    "    print(\"Training LSTM model...\")\n",
    "    \n",
    "    # Initialize model parameters\n",
    "    src_vocab_size = len(en_tokenizer.token_to_id)\n",
    "    tgt_vocab_size = len(ur_tokenizer.token_to_id)\n",
    "    emb_dim = 256\n",
    "    enc_hid_dim = 512\n",
    "    dec_hid_dim = 512\n",
    "    n_layers = 2\n",
    "    enc_dropout = 0.5\n",
    "    dec_dropout = 0.5\n",
    "    \n",
    "    # Initialize encoder, attention, and decoder\n",
    "    encoder = Encoder(src_vocab_size, emb_dim, enc_hid_dim, n_layers, enc_dropout).to(device)\n",
    "    attention = Attention(enc_hid_dim, dec_hid_dim).to(device)\n",
    "    decoder = Decoder(tgt_vocab_size, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dec_dropout, attention).to(device)\n",
    "    \n",
    "    # Initialize model\n",
    "    lstm_model = LSTMSeq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    # Initialize criterion\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_valid_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Initialize lists for learning curves\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(lstm_model, train_loader, optimizer, criterion, 1.0, device, 0.5)\n",
    "        \n",
    "        # Evaluate\n",
    "        valid_loss = evaluate(lstm_model, dev_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(valid_loss)\n",
    "        \n",
    "        # Save losses for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        # Calculate time elapsed\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.4f} | Train PPL: {math.exp(train_loss):.4f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.4f} |  Val. PPL: {math.exp(valid_loss):.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(lstm_model.state_dict(), LSTM_MODEL_PATH)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "    \n",
    "    # Plot learning curves\n",
    "    plot_learning_curves(train_losses, valid_losses, 'lstm_learning_curves.png')\n",
    "    \n",
    "    # Load best model\n",
    "    lstm_model.load_state_dict(torch.load(LSTM_MODEL_PATH))\n",
    "    \n",
    "    return lstm_model\n",
    "\n",
    "def evaluate_models(transformer, lstm_model, test_loader, test_pairs, en_tokenizer, ur_tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluate models on test set\n",
    "    \"\"\"\n",
    "    print(\"Evaluating models...\")\n",
    "    \n",
    "    # Initialize criterion\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # Evaluate transformer model\n",
    "    transformer_loss = evaluate(transformer, test_loader, criterion, device)\n",
    "    transformer_ppl = math.exp(transformer_loss)\n",
    "    transformer_bleu = calculate_bleu(test_pairs[:100], en_tokenizer, ur_tokenizer, transformer, device)\n",
    "    transformer_rouge = calculate_rouge(test_pairs[:100], en_tokenizer, ur_tokenizer, transformer, device)\n",
    "    \n",
    "    # Evaluate LSTM model\n",
    "    lstm_loss = evaluate(lstm_model, test_loader, criterion, device)\n",
    "    lstm_ppl = math.exp(lstm_loss)\n",
    "    lstm_bleu = calculate_bleu(test_pairs[:100], en_tokenizer, ur_tokenizer, lstm_model, device)\n",
    "    lstm_rouge = calculate_rouge(test_pairs[:100], en_tokenizer, ur_tokenizer, lstm_model, device)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Transformer Model:\")\n",
    "    print(f\"Test Loss: {transformer_loss:.4f} | Test PPL: {transformer_ppl:.4f}\")\n",
    "    print(f\"BLEU Score: {transformer_bleu:.4f}\")\n",
    "    print(f\"ROUGE Scores: {transformer_rouge}\")\n",
    "    \n",
    "    print(\"\\nLSTM Model:\")\n",
    "    print(f\"Test Loss: {lstm_loss:.4f} | Test PPL: {lstm_ppl:.4f}\")\n",
    "    print(f\"BLEU Score: {lstm_bleu:.4f}\")\n",
    "    print(f\"ROUGE Scores: {lstm_rouge}\")\n",
    "    \n",
    "    # Compare models\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(f\"Loss Difference: {abs(transformer_loss - lstm_loss):.4f}\")\n",
    "    print(f\"PPL Difference: {abs(transformer_ppl - lstm_ppl):.4f}\")\n",
    "    print(f\"BLEU Difference: {abs(transformer_bleu - lstm_bleu):.4f}\")\n",
    "    \n",
    "    # Create results table\n",
    "    results = {\n",
    "        'Model': ['Transformer', 'LSTM'],\n",
    "        'Loss': [transformer_loss, lstm_loss],\n",
    "        'Perplexity': [transformer_ppl, lstm_ppl],\n",
    "        'BLEU': [transformer_bleu, lstm_bleu],\n",
    "        'ROUGE-1': [transformer_rouge['rouge1'], lstm_rouge['rouge1']],\n",
    "        'ROUGE-2': [transformer_rouge['rouge2'], lstm_rouge['rouge2']],\n",
    "        'ROUGE-L': [transformer_rouge['rougeL'], lstm_rouge['rougeL']]\n",
    "    }\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nResults Table:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('model_comparison.csv', index=False)\n",
    "\n",
    "class TranslationGUI:\n",
    "    \"\"\"\n",
    "    GUI for English to Urdu translation\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer, lstm_model, en_tokenizer, ur_tokenizer):\n",
    "        self.transformer = transformer\n",
    "        self.lstm_model = lstm_model\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.ur_tokenizer = ur_tokenizer\n",
    "        self.current_model = transformer  # Default to transformer\n",
    "        \n",
    "        # Initialize Tkinter root\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"English to Urdu Translator\")\n",
    "        self.root.geometry(\"800x600\")\n",
    "        \n",
    "        # Create frame for model selection\n",
    "        self.model_frame = tk.Frame(self.root)\n",
    "        self.model_frame.pack(pady=10)\n",
    "        \n",
    "        # Create model selection radio buttons\n",
    "        self.model_var = tk.StringVar(value=\"transformer\")\n",
    "        tk.Radiobutton(self.model_frame, text=\"Transformer\", variable=self.model_var, value=\"transformer\", command=self.update_model).pack(side=tk.LEFT, padx=10)\n",
    "        tk.Radiobutton(self.model_frame, text=\"LSTM\", variable=self.model_var, value=\"lstm\", command=self.update_model).pack(side=tk.LEFT, padx=10)\n",
    "        \n",
    "        # Create input label and text entry\n",
    "        tk.Label(self.root, text=\"Enter English text:\").pack(anchor=tk.W, padx=10, pady=5)\n",
    "        self.input_entry = tk.Entry(self.root, width=80)\n",
    "        self.input_entry.pack(padx=10, pady=5, fill=tk.X)\n",
    "        self.input_entry.bind(\"<Return>\", self.translate)\n",
    "        \n",
    "        # Create translate button\n",
    "        self.translate_button = tk.Button(self.root, text=\"Translate\", command=self.translate)\n",
    "        self.translate_button.pack(pady=10)\n",
    "        \n",
    "        # Create conversation history text widget\n",
    "        self.conversation = scrolledtext.ScrolledText(self.root, wrap=tk.WORD, width=80, height=20)\n",
    "        self.conversation.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "        self.conversation.tag_configure(\"english\", justify=tk.LEFT)\n",
    "        self.conversation.tag_configure(\"urdu\", justify=tk.RIGHT)\n",
    "        \n",
    "        # Create attention visualization button\n",
    "        self.attention_button = tk.Button(self.root, text=\"Visualize Attention\", command=self.visualize_attention)\n",
    "        self.attention_button.pack(pady=10)\n",
    "    \n",
    "    def update_model(self):\n",
    "        \"\"\"\n",
    "        Update the current model based on radio button selection\n",
    "        \"\"\"\n",
    "        if self.model_var.get() == \"transformer\":\n",
    "            self.current_model = self.transformer\n",
    "        else:\n",
    "            self.current_model = self.lstm_model\n",
    "    \n",
    "    def translate(self, event=None):\n",
    "        \"\"\"\n",
    "        Translate input text and update conversation history\n",
    "        \"\"\"\n",
    "        # Get input text\n",
    "        input_text = self.input_entry.get()\n",
    "        \n",
    "        if not input_text:\n",
    "            return\n",
    "        \n",
    "        # Add input text to conversation\n",
    "        self.conversation.insert(tk.END, f\"{input_text}\\n\", \"english\")\n",
    "        \n",
    "        # Translate text\n",
    "        translation = translate_sentence(self.current_model, input_text, self.en_tokenizer, self.ur_tokenizer, device)\n",
    "        \n",
    "        # Add translation to conversation\n",
    "        self.conversation.insert(tk.END, f\"{translation}\\n\\n\", \"urdu\")\n",
    "        \n",
    "        # Clear input entry\n",
    "        self.input_entry.delete(0, tk.END)\n",
    "        \n",
    "        # Scroll to bottom\n",
    "        self.conversation.see(tk.END)\n",
    "    \n",
    "    def visualize_attention(self):\n",
    "        \"\"\"\n",
    "        Visualize attention weights for the last translated sentence\n",
    "        \"\"\"\n",
    "        # Get the last English text from conversation\n",
    "        text = self.conversation.get(\"1.0\", tk.END)\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        \n",
    "        if len(lines) >= 2:\n",
    "            last_english = lines[-3]  # Skip the translation and empty line\n",
    "            \n",
    "            # Only works with Transformer model\n",
    "            if isinstance(self.current_model, Transformer):\n",
    "                fig = visualize_attention(self.current_model, last_english, self.en_tokenizer, self.ur_tokenizer, device)\n",
    "                \n",
    "                if fig:\n",
    "                    fig.show()\n",
    "                else:\n",
    "                    print(\"Error: Could not visualize attention\")\n",
    "            else:\n",
    "                print(\"Attention visualization is only available for Transformer model\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the GUI main loop\n",
    "        \"\"\"\n",
    "        self.root.mainloop()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "    # Create directory for saving models\n",
    "    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    train_loader, dev_loader, test_loader, test_pairs, en_tokenizer, ur_tokenizer = load_and_preprocess_data()\n",
    "    \n",
    "    # Check if models exist\n",
    "    transformer_exists = os.path.exists(TRANSFORMER_MODEL_PATH)\n",
    "    lstm_exists = os.path.exists(LSTM_MODEL_PATH)\n",
    "    \n",
    "    # Initialize models\n",
    "    src_vocab_size = len(en_tokenizer.token_to_id)\n",
    "    tgt_vocab_size = len(ur_tokenizer.token_to_id)\n",
    "    \n",
    "    # Initialize transformer model\n",
    "    transformer = Transformer(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        d_model=EMBEDDING_SIZE,\n",
    "        nhead=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        dim_feedforward=FFN_DIM,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize LSTM model\n",
    "    emb_dim = 256\n",
    "    enc_hid_dim = 512\n",
    "    dec_hid_dim = 512\n",
    "    n_layers = 2\n",
    "    enc_dropout = 0.5\n",
    "    dec_dropout = 0.5\n",
    "    \n",
    "    encoder = Encoder(src_vocab_size, emb_dim, enc_hid_dim, n_layers, enc_dropout).to(device)\n",
    "    attention = Attention(enc_hid_dim, dec_hid_dim).to(device)\n",
    "    decoder = Decoder(tgt_vocab_size, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dec_dropout, attention).to(device)\n",
    "    lstm_model = LSTMSeq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Load trained models if they exist, otherwise train them\n",
    "    if transformer_exists:\n",
    "        print(\"Loading trained transformer model...\")\n",
    "        transformer.load_state_dict(torch.load(TRANSFORMER_MODEL_PATH))\n",
    "    else:\n",
    "        print(\"Training transformer model...\")\n",
    "        transformer = train_transformer_model(train_loader, dev_loader, en_tokenizer, ur_tokenizer)\n",
    "    \n",
    "    if lstm_exists:\n",
    "        print(\"Loading trained LSTM model...\")\n",
    "        lstm_model.load_state_dict(torch.load(LSTM_MODEL_PATH))\n",
    "    else:\n",
    "        print(\"Training LSTM model...\")\n",
    "        lstm_model = train_lstm_model(train_loader, dev_loader, en_tokenizer, ur_tokenizer)\n",
    "    # Evaluate models on test set\n",
    "    print(\"Evaluating models on test set...\")\n",
    "    evaluate_models(transformer, lstm_model, test_loader, test_pairs, en_tokenizer, ur_tokenizer)\n",
    "    \n",
    "    # Create and run GUI\n",
    "    print(\"Launching translation GUI...\")\n",
    "    gui = TranslationGUI(transformer, lstm_model, en_tokenizer, ur_tokenizer)\n",
    "    gui.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
